---
layout: post
title: "Understanding Value Functions in Reinforcement Learning"
date: 2024-10-29
categories: reinforcement-learning
---

# Value Functions in Reinforcement Learning

In reinforcement learning, value functions help quantify how good it is for an agent to be in a particular state or to take a specific action in that state. They guide the agent toward its goal by providing a way to evaluate the long-term benefits of different states and actions.

## Types of Value Functions

There are two main types of value functions:

1. **State-Value Function \( V(s) \)**: Measures the expected reward an agent can obtain from a given state by following a policy \( \pi \).
2. **Action-Value Function \( Q(s, a) \)**: Measures the expected reward of taking action \( a \) in state \( s \) and then following a policy \( \pi \).

### State-Value Function

The **state-value function** \( V(s) \) for a policy \( \pi \) is defined as:

\[
V(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t r_{t} \mid s_0 = s \right]
\]

where:
- \( s \): Current state
- \( r_t \): Reward received at time step \( t \)
- \( \gamma \): Discount factor (between 0 and 1)
- \( \mathbb{E}_\pi \): Expected value when following policy \( \pi \)

This function tells us the total expected reward an agent can achieve starting from state \( s \) and following the policy \( \pi \) in the future.

### Action-Value Function

The **action-value function** \( Q(s, a) \) for a policy \( \pi \) is defined as:

\[
Q(s, a) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t r_{t} \mid s_0 = s, a_0 = a \right]
\]

Here:
- \( a \): Action taken in state \( s \)

The action-value function gives the expected reward of taking action \( a \) in state \( s \) and then following the policy \( \pi \) onward.

### The Bellman Equation

Value functions satisfy a fundamental recursive relationship called the **Bellman equation**. For the state-value function, the Bellman equation is:
\
V(s) = \sum_{a} \pi(a | s) \sum_{s'} p(s' | s, a) \left[ r + \gamma V(s') \right]
\
'''

For the action-value function, the Bellman equation is:

\
Q(s, a) = \sum_{s'} p(s' | s, a) \left[ r + \gamma \sum_{a'} \pi(a' | s') Q(s', a') \right]
\

These equations allow us to compute the values of states and actions recursively by breaking down future rewards in terms of immediate rewards and the values of subsequent states.

## Conclusion

Value functions are a core concept in reinforcement learning, enabling an agent to make decisions that maximize cumulative rewards over time. By calculating the value of states and actions, the agent can choose a policy that leads to the best long-term outcomes.

---

Hope this explanation gives you a clear understanding of value functions in RL. Stay tuned for more insights!

